<p align="center">
  <img src="assets/logo.png" alt="Observable Agent" width="600"/>
</p>

<p align="center">
  <strong>Unopinionated contract-based verification for AI agents.</strong>
</p>

<p align="center">
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.11+-blue.svg" alt="Python 3.11+"></a>
  <a href="https://github.com/kavishsathia/observable-agent/blob/main/LICENSE"><img src="https://img.shields.io/badge/license-MIT-green.svg" alt="License: MIT"></a>
  <a href="https://github.com/kavishsathia/observable-agent"><img src="https://img.shields.io/badge/coverage-82%25-brightgreen.svg" alt="Coverage: 82%"></a>
</p>

---

## The Problem

Typical deterministic software uses tools like unit tests to ensure the code functions correctly. However, as we find ourselves becoming more reliant on AI agents to do our work, we will need a smarter and more efficient means of verifying their output is correct. To fix this, this library introduces a mental model known as the Agentic Contract Framework.

Its primary function is to produce a contract with a set of commitments before the agents execution (this contract can be hardcoded, or be dynamically generated by the agent itself). Each commitment on a contract has an attached verifier, and this verifier can be set by you, the developer. If you deem a commitment can be deterministically verified, you are welcome to create a function for that (like a unit test). Otherwise, you can rely on the default semantic verifier that uses another agent to verify the correctness of the output. All the evaluations done will be collected and synced with Datadog.

## Quick Start

```python
from observable_agent import ObservableAgent, Contract, Commitment

# Define what the agent must do
contract = Contract(commitments=[
    Commitment(
        name="no_harmful_content",
        terms="The agent must not produce harmful or offensive content"
    ),
    Commitment(
        name="stay_on_topic",
        terms="The agent must only discuss topics related to the user's query"
    )
])

# Create the agent (wraps Google ADK)
agent = ObservableAgent(
    name="my_agent",
    model="gemini-2.0-flash",
    instruction="You are a helpful assistant.",
    description="A helpful assistant",
    contract=contract,
    on_implementation_complete=lambda verifier: print(verifier.verify())
)
```

## Progressive Hardening

Using this library is very much a process of continuous exploration, you observe your agents, determine their failure modes and progressively "harden" your rules. If you discover that your agent commonly does a certain mistake, you can simply create a commitment to not do that mistake and add a deterministic verifier to help catch it 100% of the times. I would personally recommend just starting with the default semantic verifier and understanding the failure modes of your agent in your domain first!

## Key Concepts

## Installation

```bash

```
